from __future__ import annotations

import argparse
import re
import sys
import textwrap
import urllib.parse
import urllib.request
import zipfile
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, Optional

KAGGLE_URL_RE = re.compile(r"https?://(?:www\.)?kaggle\.com/[^\s\)\]]+", re.IGNORECASE)
DROPBOX_URL_RE = re.compile(r"https?://(?:www\.)?dropbox\.com/[^\s\)\]]+", re.IGNORECASE)

AUTOGEN_START = "<!-- AUTOGENERATED: kaggle_sync_readmes.py START -->"
AUTOGEN_END = "<!-- AUTOGENERATED: kaggle_sync_readmes.py END -->"


@dataclass(frozen=True)
class KaggleRef:
    kind: str  # dataset | competition | kernel
    ref: str
    url: str


def _iter_kaggle_subfolders(data_dir: Path) -> Iterable[Path]:
    for folder in sorted(data_dir.glob("KAGGLE_*")):
        if folder.is_dir():
            yield folder


def _extract_urls(readme_text: str) -> tuple[list[str], list[str]]:
    kaggle_urls = sorted(set(KAGGLE_URL_RE.findall(readme_text)))
    dropbox_urls = sorted(set(DROPBOX_URL_RE.findall(readme_text)))
    return kaggle_urls, dropbox_urls


def _parse_kaggle_url(url: str) -> Optional[KaggleRef]:
    u = url.strip().rstrip("/")

    # datasets: https://www.kaggle.com/datasets/<owner>/<slug>
    m = re.search(r"kaggle\.com/datasets/([^/]+)/([^/?#]+)", u, flags=re.IGNORECASE)
    if m:
        return KaggleRef(kind="dataset", ref=f"{m.group(1)}/{m.group(2)}", url=url)

    # competitions: https://www.kaggle.com/competitions/<slug>
    m = re.search(r"kaggle\.com/competitions/([^/?#]+)", u, flags=re.IGNORECASE)
    if m:
        return KaggleRef(kind="competition", ref=m.group(1), url=url)

    # code (kernels): https://www.kaggle.com/code/<owner>/<slug>
    m = re.search(r"kaggle\.com/code/([^/]+)/([^/?#]+)", u, flags=re.IGNORECASE)
    if m:
        return KaggleRef(kind="kernel", ref=f"{m.group(1)}/{m.group(2)}", url=url)

    return None


def _human_bytes(n: Optional[int]) -> str:
    if not n:
        return "unknown"
    units = ["B", "KB", "MB", "GB", "TB"]
    x = float(n)
    for unit in units:
        if x < 1024.0:
            return f"{x:,.1f} {unit}"
        x /= 1024.0
    return f"{x:,.1f} PB"


def _unwrap_markdown(s: Optional[str]) -> str:
    return (s or "").strip()


def _zip_extract_all(zip_path: Path, extract_to: Path) -> None:
    extract_to.mkdir(parents=True, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(extract_to)


def _download_url(url: str, dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
    with urllib.request.urlopen(req) as resp:
        dest.write_bytes(resp.read())


def _dropbox_direct_download(url: str) -> str:
    parsed = urllib.parse.urlparse(url)
    qs = urllib.parse.parse_qs(parsed.query)
    qs["dl"] = ["1"]
    query = urllib.parse.urlencode(qs, doseq=True)
    return urllib.parse.urlunparse(parsed._replace(query=query))


def _upsert_autogen_section(readme_text: str, block: str) -> str:
    if AUTOGEN_START in readme_text and AUTOGEN_END in readme_text:
        pre = readme_text.split(AUTOGEN_START, 1)[0].rstrip()
        post = readme_text.split(AUTOGEN_END, 1)[1].lstrip()
        return f"{pre}\n\n{AUTOGEN_START}\n{block}{AUTOGEN_END}\n\n{post}".rstrip() + "\n"

    return readme_text.rstrip() + "\n\n" + AUTOGEN_START + "\n" + block + AUTOGEN_END + "\n"


def _safe_kernels_view(api, kernel_ref: str):
    try:
        return api.kernels_view(kernel_ref)
    except TypeError:
        owner, slug = kernel_ref.split("/", 1)
        return api.kernels_view(owner=owner, kernel_slug=slug)


def _render_block(
    folder: Path,
    kaggle_refs: list[KaggleRef],
    dropbox_urls: list[str],
    api,
    authenticated: bool,
    download: bool,
) -> str:
    now = datetime.now(timezone.utc).isoformat(timespec="seconds")
    lines: list[str] = []

    lines.append("## Dataset / Problem (autogenerated)")
    lines.append("")
    lines.append(f"- Folder: `{folder.name}`")
    lines.append(f"- Updated: `{now}`")
    lines.append("")

    if not kaggle_refs and not dropbox_urls:
        lines.append("No source URLs found in this README.")
        return "\n".join(lines).rstrip() + "\n"

    lines.append("### Sources")
    for ref in kaggle_refs:
        lines.append(f"- [{ref.url}]({ref.url})  (`{ref.kind}`: `{ref.ref}`)")
    for url in dropbox_urls:
        lines.append(f"- [{url}]({url})  (`external`: `dropbox`)")
    lines.append("")

    if not authenticated:
        lines.append("> Note: Kaggle API is not authenticated in this environment.\n"
                    "> This section will include only URL parsing (no Kaggle descriptions / downloads).\n"
                    "> Configure `~/.kaggle/kaggle.json` and rerun the script.")
        lines.append("")

    dataset_refs: set[str] = set(r.ref for r in kaggle_refs if r.kind == "dataset")
    competition_refs: set[str] = set(r.ref for r in kaggle_refs if r.kind == "competition")
    kernel_refs: list[str] = [r.ref for r in kaggle_refs if r.kind == "kernel"]

    kernel_summaries: list[tuple[str, str]] = []

    if authenticated and api is not None:
        for kref in kernel_refs:
            try:
                kv = _safe_kernels_view(api, kref)
                title = getattr(kv, "title", "") or ""
                kernel_summaries.append((kref, title))
                for ds in getattr(kv, "datasetDataSources", []) or []:
                    dataset_refs.add(ds)
                for comp in getattr(kv, "competitionDataSources", []) or []:
                    competition_refs.add(comp)
            except Exception as e:
                kernel_summaries.append((kref, f"(failed to resolve kernel metadata: {e})"))

    if kernel_refs:
        lines.append("### Kaggle notebooks (context)")
        for kref in kernel_refs:
            title = next((t for r, t in kernel_summaries if r == kref), "")
            if title:
                lines.append(f"- `{kref}` — {title}")
            else:
                lines.append(f"- `{kref}`")
        lines.append("")

    if dataset_refs:
        lines.append("### Dataset description")
        for ds_ref in sorted(dataset_refs):
            if not authenticated or api is None:
                lines.append(f"- `{ds_ref}` (Kaggle dataset) — https://www.kaggle.com/datasets/{ds_ref}")
                continue

            try:
                dv = api.dataset_view(ds_ref)
                title = getattr(dv, "title", "") or ds_ref
                subtitle = getattr(dv, "subtitle", "") or ""
                desc = _unwrap_markdown(getattr(dv, "description", "") or "")
                total_bytes = getattr(dv, "totalBytes", None)
                license_name = getattr(dv, "licenseName", None)

                lines.append(f"#### `{ds_ref}` — {title}")
                if subtitle:
                    lines.append(f"- Subtitle: {subtitle}")
                lines.append(f"- URL: https://www.kaggle.com/datasets/{ds_ref}")
                lines.append(f"- Size: {_human_bytes(total_bytes)}")
                if license_name:
                    lines.append(f"- License: {license_name}")

                # File listing (best-effort)
                try:
                    fl = api.dataset_list_files(ds_ref)
                    files = getattr(fl, "files", []) or []
                    if files:
                        lines.append("- Files (sample):")
                        for f in files[:20]:
                            name = getattr(f, "name", "")
                            size = getattr(f, "totalBytes", None)
                            if name:
                                lines.append(f"  - {name} ({_human_bytes(size)})")
                        if len(files) > 20:
                            lines.append(f"  - … +{len(files) - 20} more")
                except Exception:
                    pass

                if desc:
                    lines.append("")
                    lines.append("**Description (Kaggle):**")
                    lines.append("")
                    lines.append(desc)
                lines.append("")
            except Exception as e:
                lines.append(f"- `{ds_ref}` (failed to fetch metadata: {e})")
        lines.append("")

    if competition_refs:
        lines.append("### Problem description")
        for comp in sorted(competition_refs):
            if not authenticated or api is None:
                lines.append(f"- `{comp}` (Kaggle competition) — https://www.kaggle.com/competitions/{comp}")
                continue

            try:
                cv = api.competition_view(comp)
                title = getattr(cv, "title", "") or comp
                desc = _unwrap_markdown(getattr(cv, "description", "") or "")

                lines.append(f"#### `{comp}` — {title}")
                lines.append(f"- URL: https://www.kaggle.com/competitions/{comp}")
                if desc:
                    lines.append("")
                    lines.append("**Description (Kaggle):**")
                    lines.append("")
                    lines.append(desc)
                lines.append("")
            except Exception as e:
                lines.append(f"- `{comp}` (failed to fetch metadata: {e})")
        lines.append("")

    if dropbox_urls:
        lines.append("### External data (Dropbox)")
        for url in dropbox_urls:
            lines.append(f"- {url}")
        lines.append("")

    if download:
        lines.append("### Local download target")
        lines.append(f"- `{(folder / 'raw').as_posix()}`")
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(
        description="Download Kaggle MTA datasets referenced in data/KAGGLE_*/README.md and enrich README descriptions.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent(
            """
            Requirements:
              - `pip install kaggle`
              - Kaggle API token at `~/.kaggle/kaggle.json`

            Output:
              - Downloads to `data/KAGGLE_*/raw/`
              - Updates each README.md with an autogenerated section between markers.
            """
        ).strip(),
    )
    parser.add_argument("--repo-root", type=Path, default=None, help="Repository root (defaults to parent of tools/)")
    parser.add_argument("--data-dir", type=Path, default=None, help="Data directory (defaults to <repo-root>/data)")
    parser.add_argument("--no-download", action="store_true", help="Do not download datasets; only enrich READMEs")

    args = parser.parse_args(argv)

    repo_root = args.repo_root or Path(__file__).resolve().parents[1]
    data_dir = args.data_dir or (repo_root / "data")
    download = not args.no_download

    if not data_dir.exists():
        print(f"ERROR: data dir not found: {data_dir}", file=sys.stderr)
        return 2

    try:
        from kaggle.api.kaggle_api_extended import KaggleApi

        api = KaggleApi()
        api.authenticate()
        authenticated = True
    except Exception as e:
        api = None
        authenticated = False
        print(f"WARNING: Kaggle API not available/authenticated ({e}). Continuing best-effort.", file=sys.stderr)

    for folder in _iter_kaggle_subfolders(data_dir):
        readme_path = folder / "README.md"
        if not readme_path.exists():
            continue

        readme_text = readme_path.read_text(encoding="utf-8")
        kaggle_urls, dropbox_urls = _extract_urls(readme_text)
        kaggle_refs = [r for r in (_parse_kaggle_url(u) for u in kaggle_urls) if r is not None]

        # Best-effort downloads
        if download:
            raw_dir = folder / "raw"
            if authenticated and api is not None:
                dataset_refs: set[str] = set(r.ref for r in kaggle_refs if r.kind == "dataset")
                competition_refs: set[str] = set(r.ref for r in kaggle_refs if r.kind == "competition")

                for r in kaggle_refs:
                    if r.kind == "kernel":
                        try:
                            kv = _safe_kernels_view(api, r.ref)
                            for ds in getattr(kv, "datasetDataSources", []) or []:
                                dataset_refs.add(ds)
                            for comp in getattr(kv, "competitionDataSources", []) or []:
                                competition_refs.add(comp)
                        except Exception:
                            pass

                for ds_ref in sorted(dataset_refs):
                    out_dir = raw_dir / "datasets" / ds_ref.replace("/", "__")
                    out_dir.mkdir(parents=True, exist_ok=True)
                    try:
                        api.dataset_download_files(ds_ref, path=str(out_dir), quiet=False, unzip=False)
                        for zip_path in out_dir.glob("*.zip"):
                            _zip_extract_all(zip_path, out_dir)
                    except Exception as e:
                        print(f"WARNING: failed to download dataset {ds_ref}: {e}", file=sys.stderr)

                for comp in sorted(competition_refs):
                    out_dir = raw_dir / "competitions" / comp
                    out_dir.mkdir(parents=True, exist_ok=True)
                    try:
                        api.competition_download_files(comp, path=str(out_dir), quiet=False)
                        for zip_path in out_dir.glob("*.zip"):
                            _zip_extract_all(zip_path, out_dir)
                    except Exception as e:
                        print(f"WARNING: failed to download competition {comp}: {e}", file=sys.stderr)

            for url in dropbox_urls:
                try:
                    direct = _dropbox_direct_download(url)
                    out_dir = raw_dir / "external"
                    out_dir.mkdir(parents=True, exist_ok=True)
                    out_file = out_dir / "dropbox_download"
                    _download_url(direct, out_file)
                except Exception as e:
                    print(f"WARNING: failed to download dropbox url {url}: {e}", file=sys.stderr)

        block = _render_block(
            folder=folder,
            kaggle_refs=kaggle_refs,
            dropbox_urls=dropbox_urls,
            api=api,
            authenticated=authenticated,
            download=download,
        )
        updated = _upsert_autogen_section(readme_text, block)
        readme_path.write_text(updated, encoding="utf-8")
        print(f"Updated {readme_path.relative_to(repo_root)} (kaggle urls: {len(kaggle_refs)}, dropbox urls: {len(dropbox_urls)})")

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
